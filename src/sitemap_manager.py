"""
Sitemap Manager Module

Handles interactive sitemap download, parsing, and management with retry logic.
Supports sitemap indices and selective sitemap downloads.
"""

import requests
from lxml import etree
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import logging
from tqdm import tqdm
import hashlib
import time

logger = logging.getLogger(__name__)


class SitemapManager:
    """
    Manages sitemap downloads and parsing with interactive user input.
    
    Features:
    - Interactive sitemap URL input
    - Automatic detection of sitemap indices
    - Selective download of sub-sitemaps
    - Retry logic with 10 attempts
    - Local caching to avoid re-downloads
    - Progress tracking for large sitemaps
    """
    
    def __init__(self, sitemap_dir: str = "sitemaps"):
        """
        Initialize SitemapManager.
        
        Args:
            sitemap_dir: Directory to store downloaded sitemaps
        """
        self.sitemap_dir = Path(sitemap_dir)
        self.sitemap_dir.mkdir(exist_ok=True)
        logger.info(f"Sitemap directory: {self.sitemap_dir}")
    
    def _get_cache_filename(self, url: str) -> Path:
        """
        Generate a cache filename based on URL hash.
        
        Args:
            url: Sitemap URL
            
        Returns:
            Path to cache file
        """
        # Create a hash of the URL for filename
        url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
        
        # Extract domain for readable filename
        from urllib.parse import urlparse
        domain = urlparse(url).netloc.replace('www.', '')
        
        filename = f"{domain}_{url_hash}.xml"
        return self.sitemap_dir / filename
    
    def _download_with_retry(
        self,
        url: str,
        max_retries: int = 10,
        timeout: int = 30
    ) -> Optional[bytes]:
        """
        Download sitemap with retry logic.
        
        Args:
            url: URL to download
            max_retries: Maximum number of retry attempts
            timeout: Timeout for each request in seconds
            
        Returns:
            Downloaded content as bytes, or None if all retries failed
        """
        print(f"\nüì• Downloading sitemap: {url}")
        
        for attempt in range(1, max_retries + 1):
            try:
                print(f"   Attempt {attempt}/{max_retries}...", end=" ")
                
                response = requests.get(url, timeout=timeout)
                response.raise_for_status()
                
                print("‚úÖ Success!")
                logger.info(f"Downloaded sitemap from {url} (attempt {attempt})")
                return response.content
                
            except requests.RequestException as e:
                print(f"‚ùå Failed: {str(e)[:50]}")
                logger.warning(f"Download attempt {attempt} failed: {str(e)}")
                
                if attempt < max_retries:
                    # Exponential backoff
                    wait_time = min(2 ** attempt, 30)
                    print(f"   Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                else:
                    print(f"\n‚ùå All {max_retries} download attempts failed!")
                    return None
        
        return None
    
    def _parse_sitemap_content(self, content: bytes) -> Tuple[List[str], List[str]]:
        """
        Parse sitemap XML content.
        
        Args:
            content: XML content as bytes
            
        Returns:
            Tuple of (urls, sub_sitemaps)
        """
        try:
            root = etree.fromstring(content)
            namespaces = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            
            # Extract URLs
            urls = root.xpath('//ns:url/ns:loc/text()', namespaces=namespaces)
            
            # Extract sub-sitemaps (sitemap index)
            sub_sitemaps = root.xpath('//ns:sitemap/ns:loc/text()', namespaces=namespaces)
            
            return urls, sub_sitemaps
            
        except etree.XMLSyntaxError as e:
            logger.error(f"XML parsing error: {str(e)}")
            return [], []
    
    def get_sitemap_url_interactive(self) -> str:
        """
        Prompt user for sitemap URL interactively.
        
        Returns:
            Sitemap URL entered by user
        """
        print("\n" + "="*60)
        print("üó∫Ô∏è  SITEMAP CONFIGURATION")
        print("="*60)
        
        while True:
            url = input("\nEnter your sitemap URL (e.g., https://example.com/sitemap.xml): ").strip()
            
            if not url:
                print("‚ùå URL cannot be empty. Please try again.")
                continue
            
            if not url.startswith(('http://', 'https://')):
                print("‚ùå URL must start with http:// or https://")
                continue
            
            return url
    
    def select_sitemaps_interactive(self, sitemap_urls: List[str]) -> List[str]:
        """
        Let user select which sitemaps to download from a list.
        
        Args:
            sitemap_urls: List of sitemap URLs
            
        Returns:
            Selected sitemap URLs
        """
        print("\n" + "="*60)
        print(f"üìã FOUND {len(sitemap_urls)} SUB-SITEMAPS")
        print("="*60)
        
        for idx, url in enumerate(sitemap_urls, 1):
            # Extract readable name from URL
            name = url.split('/')[-1].replace('.xml', '').replace('sitemap', '')
            print(f"  [{idx}] {name or 'main'} - {url}")
        
        print("\n" + "-"*60)
        print("Selection options:")
        print("  - Enter numbers separated by commas (e.g., 1,3,5)")
        print("  - Enter 'all' to download all sitemaps")
        print("  - Enter 'none' to skip")
        print("-"*60)
        
        while True:
            choice = input("\nYour selection: ").strip().lower()
            
            if choice == 'all':
                print(f"‚úÖ Selected all {len(sitemap_urls)} sitemaps")
                return sitemap_urls
            
            if choice == 'none':
                print("‚è≠Ô∏è  Skipped sitemap selection")
                return []
            
            # Parse comma-separated numbers
            try:
                indices = [int(x.strip()) for x in choice.split(',')]
                
                # Validate indices
                if all(1 <= idx <= len(sitemap_urls) for idx in indices):
                    selected = [sitemap_urls[idx-1] for idx in indices]
                    print(f"‚úÖ Selected {len(selected)} sitemap(s)")
                    return selected
                else:
                    print(f"‚ùå Invalid selection. Numbers must be between 1 and {len(sitemap_urls)}")
            
            except ValueError:
                print("‚ùå Invalid format. Use comma-separated numbers or 'all'/'none'")
    
    def download_and_parse_sitemap(
        self,
        url: Optional[str] = None,
        force_download: bool = False
    ) -> List[str]:
        """
        Download and parse sitemap with caching and interactive features.
        
        Args:
            url: Sitemap URL (prompts user if not provided)
            force_download: Force re-download even if cached
            
        Returns:
            List of URLs extracted from sitemap(s)
        """
        # Get URL from user if not provided
        if not url:
            url = self.get_sitemap_url_interactive()
        
        # Check cache first
        cache_file = self._get_cache_filename(url)
        
        if cache_file.exists() and not force_download:
            print(f"\n‚úÖ Using cached sitemap: {cache_file.name}")
            
            retry = input("   Download again? (y/N): ").strip().lower()
            if retry not in ['y', 'yes']:
                with open(cache_file, 'rb') as f:
                    content = f.read()
                
                urls, sub_sitemaps = self._parse_sitemap_content(content)
                
                if sub_sitemaps:
                    # Handle sitemap index
                    return self._handle_sitemap_index(sub_sitemaps)
                
                print(f"   üìä Loaded {len(urls)} URLs from cache")
                return urls
        
        # Download sitemap
        content = self._download_with_retry(url)
        
        if not content:
            print("\n‚ö†Ô∏è  Failed to download sitemap.")
            
            retry = input("Do you want to try again? (y/N): ").strip().lower()
            if retry in ['y', 'yes']:
                return self.download_and_parse_sitemap(url, force_download=True)
            else:
                print("‚ùå Aborting due to sitemap download failure.")
                return []
        
        # Save to cache
        with open(cache_file, 'wb') as f:
            f.write(content)
        print(f"üíæ Cached sitemap: {cache_file.name}")
        
        # Parse content
        urls, sub_sitemaps = self._parse_sitemap_content(content)
        
        # Handle sitemap index
        if sub_sitemaps:
            return self._handle_sitemap_index(sub_sitemaps)
        
        print(f"‚úÖ Extracted {len(urls)} URLs from sitemap")
        logger.info(f"Parsed {len(urls)} URLs from sitemap")
        
        return urls
    
    def _handle_sitemap_index(self, sub_sitemaps: List[str]) -> List[str]:
        """
        Handle sitemap index by letting user select which sitemaps to download.
        
        Args:
            sub_sitemaps: List of sub-sitemap URLs
            
        Returns:
            Combined URLs from selected sitemaps
        """
        print(f"\nüîó This is a sitemap index containing {len(sub_sitemaps)} sub-sitemaps")
        
        selected_sitemaps = self.select_sitemaps_interactive(sub_sitemaps)
        
        if not selected_sitemaps:
            return []
        
        # Download and parse selected sitemaps
        all_urls = []
        
        print(f"\nüì• Downloading {len(selected_sitemaps)} sitemap(s)...")
        
        for sitemap_url in tqdm(selected_sitemaps, desc="Processing sitemaps"):
            cache_file = self._get_cache_filename(sitemap_url)
            
            # Check cache
            if cache_file.exists():
                with open(cache_file, 'rb') as f:
                    content = f.read()
            else:
                content = self._download_with_retry(sitemap_url, max_retries=3)
                
                if content:
                    with open(cache_file, 'wb') as f:
                        f.write(content)
            
            if content:
                urls, _ = self._parse_sitemap_content(content)
                all_urls.extend(urls)
        
        print(f"\n‚úÖ Total URLs extracted: {len(all_urls)}")
        logger.info(f"Extracted {len(all_urls)} URLs from {len(selected_sitemaps)} sitemaps")
        
        return all_urls

