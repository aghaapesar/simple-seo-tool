# SEO Content Analysis & Optimization Tool v2.2.3

A powerful, interactive Python application optimized for **Persian/Farsi content** that helps you improve your website's SEO through:
1. **Content Optimization**: Analyze Google Search Console data with Persian-aware AI
2. **SEO Data Collection**: Scrape and audit page titles, meta descriptions, and SEO tags
3. **Knowledge Base**: Track content history and avoid duplicates

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Persian Optimized](https://img.shields.io/badge/Persian-Optimized-green.svg)]()
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ†• What's New in v2.2.3

### Persian Language Optimization ğŸ‡®ğŸ‡· (Enhanced)
- âœ… **Persian-Aware AI Prompts**: Specialized prompts for Farsi content analysis
- âœ… **LSI Keywords**: Persian-specific related keywords suggestions
- âœ… **Search Intent**: Understanding Iranian user behavior and intent
- âœ… **Content Structure**: H2/H3 headings optimized for Persian SEO
- âœ… **Persian URL Decoding**: Proper handling of Persian URLs in scraping mode
- âœ… **Fully Persian Excel Output**: All column headers and content in Persian

### Smart Clustering & Fallback Strategy ğŸ”„
- âœ… **Intelligent Duplicate Detection**: Prevents repetitive content with adjustable thresholds
- âœ… **Fallback Clustering**: Multiple retry strategies when clustering fails
- âœ… **User-Guided Recovery**: Interactive options to adjust clustering parameters
- âœ… **Test Mode Support**: Clustering works in test mode with limited data
- âœ… **Threshold Adjustment**: Lower duplicate detection threshold on demand

### Knowledge Base System ğŸ§ 
- âœ… **Project Memory**: Track content history for each project
- âœ… **Duplicate Detection**: Automatically detect similar content to avoid repetition
- âœ… **Performance Tracking**: Compare predicted vs actual metrics
- âœ… **Smart Suggestions**: Learn from past performance to improve predictions

### Previous Features (v2.0)
- âœ… **Interactive Mode**: User-friendly prompts and selections
- âœ… **Dual Modes**: Content optimization + SEO data collection
- âœ… **Smart Sitemap Management**: Automatic caching, retry logic, selective downloads
- âœ… **Multi-File Support**: Process multiple Excel files in one run
- âœ… **Test Mode**: Quick validation with 10-item limits
- âœ… **Resume Capability**: Continue interrupted scraping sessions
- âœ… **Progress Tracking**: Real-time progress bars and status messages
- âœ… **Organized Structure**: Separate folders for input, output, and sitemaps

---

## ğŸ“‹ Table of Contents

- [English Documentation](#english-documentation)
  - [Quick Start](#quick-start)
  - [Features](#features)
  - [Installation](#installation)
  - [Usage](#usage)
  - [Modes](#modes)
  - [Troubleshooting](#troubleshooting)
- [Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙØ§Ø±Ø³ÛŒ](#Ù…Ø³ØªÙ†Ø¯Ø§Øª-ÙØ§Ø±Ø³ÛŒ)
  - [Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹](#Ø´Ø±ÙˆØ¹-Ø³Ø±ÛŒØ¹)
  - [ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§](#ÙˆÛŒÚ˜Ú¯ÛŒÙ‡Ø§)
  - [Ù†ØµØ¨](#Ù†ØµØ¨)
  - [Ø§Ø³ØªÙØ§Ø¯Ù‡](#Ø§Ø³ØªÙØ§Ø¯Ù‡)
  - [Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ø¬Ø±Ø§](#Ø­Ø§Ù„ØªÙ‡Ø§ÛŒ-Ø§Ø¬Ø±Ø§)
  - [Ø±ÙØ¹ Ù…Ø´Ú©Ù„Ø§Øª](#Ø±ÙØ¹-Ù…Ø´Ú©Ù„Ø§Øª)

---

# English Documentation

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Configure API
Edit `config.yaml` with your AI provider credentials:
```yaml
ai:
  provider: openai_compatible
  model: openai/gpt-4o-mini
  compatible_base_url: "https://ai.liara.ir/api/YOUR_PROJECT_ID/v1"
  compatible_api_key: "YOUR_API_KEY"
```

### 3. Prepare Your Data
- Copy Excel files from Google Search Console to `input/` folder
- Have your sitemap URL ready

### 4. Run the Tool
```bash
# Interactive mode (recommended for first time)
python3 main.py

# Content optimization mode directly
python3 main.py --mode content

# SEO data collection mode
python3 main.py --mode scraping

# Test mode (10 items only)
python3 main.py --mode content --test
```

---

## ğŸ¯ Features

### Mode 1: Content Optimization (Persian-Optimized)
- **Search Console Analysis**: Load and analyze Google Search Console exports
- **Persian-Aware AI**: Specialized analysis for Farsi content and Iranian users
- **LSI Keywords**: Persian-specific related keywords (Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· ÙØ§Ø±Ø³ÛŒ)
- **Search Intent Analysis**: Understanding Iranian user search behavior
- **Comprehensive Suggestions**: 
  - Content improvements with Persian SEO best practices
  - H2/H3 headings optimized for Farsi queries
  - Meta descriptions (Ø­Ø¯Ø§Ú©Ø«Ø± Û±Û¶Û° Ú©Ø§Ø±Ø§Ú©ØªØ±)
  - FAQ suggestions based on Persian search patterns
  - Internal linking with Persian anchor texts
- **Knowledge Base Integration**: 
  - Track all generated content
  - Avoid duplicate topics automatically
  - Learn from performance over time

### Mode 2: SEO Data Collection
- **Page Scraping**: Extract titles, meta descriptions, H1s from all pages
- **Batch Processing**: Scrape pages in controlled batches with pause/resume
- **Progress Tracking**: Real-time progress bars and statistics
- **Error Handling**: Automatic retry logic and graceful error management
- **Resume Capability**: Pick up where you left off if interrupted

### Common Features
- **Interactive Sitemap Management**:
  - Automatic caching (no re-downloads)
  - 10-retry logic with exponential backoff
  - Sitemap index support with selective downloads
  - User prompts for manual retry
  
- **Smart File Handling**:
  - Multi-file selection from `input/` directory
  - File metadata display (size, date)
  - Automatic backup creation
  
- **Test Mode**: Validate with 10-item limits before full run
- **Comprehensive Logging**: Detailed logs saved to `seo_optimizer.log`
- **Multiple AI Providers**: OpenAI, Azure, Anthropic, or compatible APIs

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8 or higher
- pip (Python package manager)
- Internet connection for AI API calls

### Setup
```bash
# Create project directory
cd SEOContentAnalysis

# Install dependencies
pip install -r requirements.txt

# Create config from sample
cp config.sample.yaml config.yaml

# Edit config.yaml with your credentials
nano config.yaml
```

### Directory Structure
```
SEOContentAnalysis/
â”œâ”€â”€ input/              # Place your Excel files here
â”œâ”€â”€ sitemaps/           # Downloaded sitemaps (auto-cached)
â”œâ”€â”€ output/             # Generated Excel reports
â”œâ”€â”€ logs/               # Application logs
â”œâ”€â”€ knowledge_base/     # Project memory & content history âœ¨ NEW
â”œâ”€â”€ main.py             # Main application
â”œâ”€â”€ config.yaml         # Your configuration
â””â”€â”€ src/                # Source modules
    â”œâ”€â”€ knowledge_base.py  # Knowledge base system âœ¨ NEW
    â””â”€â”€ ...
```

---

## ğŸ® Usage

### Mode 1: Content Optimization

**Purpose**: Analyze Search Console data to improve existing content and find new opportunities.

**Workflow**:
1. Export data from Google Search Console (Performance â†’ Export)
2. Copy Excel file(s) to `input/` folder
3. Run: `python3 main.py --mode content`
4. **Enter project name** (e.g., example.com) - used for knowledge base âœ¨
5. Select files when prompted
6. Enter sitemap URL when requested
7. Wait for Persian-optimized AI analysis to complete
8. Review results in `output/` folder

**What Happens Behind the Scenes**:
- AI analyzes with Persian language understanding
- Knowledge base checks for duplicate content
- Suggestions include Persian LSI keywords
- Content structure optimized for Iranian users
- All generated content tracked for future reference

**Output Files**:
- `improvements_[filename].xlsx` - Suggestions for existing pages
- `new_content_[filename].xlsx` - Ideas for new articles

**Example**:
```bash
$ python3 main.py --mode content

ğŸš€ SEO CONTENT ANALYSIS & OPTIMIZATION TOOL
============================================
Version: 2.1 | Persian AI + Knowledge Base

ğŸ“‹ PROJECT IDENTIFICATION
Enter a name for this project: example.com
âœ… Project name: example.com

ğŸ“Š FOUND 2 EXCEL FILE(S)
  [1] example-blog.xlsx (94.5 KB | 2025-10-11)
  [2] example-product.xlsx (102.1 KB | 2025-10-11)

Your selection: 1,2

ğŸ—ºï¸  SITEMAP CONFIGURATION
Enter your sitemap URL: https://example.com/sitemap.xml

[Processing with Persian-optimized AI...]

âœ… Knowledge base: No duplicate content found
âœ… Generated 15 improvement suggestions
âœ… Created 8 new content ideas with Persian structure
```

---

### Mode 2: SEO Data Collection

**Purpose**: Scrape and audit all pages in your sitemap for SEO data.

**Workflow**:
1. Run: `python3 main.py --mode scraping`
2. Enter sitemap URL when prompted
3. Choose batch size (e.g., 50 pages at a time)
4. Review each batch, continue or pause
5. Results saved to `output/seo_data_[domain].xlsx`

**Collected Data**:
- Page URL
- Title tag
- Meta description
- H1 heading
- Canonical URL
- Open Graph tags (title, description)
- Twitter Card tags

**Example**:
```bash
$ python3 main.py --mode scraping --test

ğŸ” MODE: SEO Data Collection
ğŸ§ª TEST MODE: Will scrape only 10 pages

Enter sitemap URL: https://example.com/sitemap.xml

ğŸ“¥ Downloading sitemap...
âœ… Extracted 1,250 URLs

How many pages per batch? 10

ğŸ”„ Scraping batch: 1 to 10 of 10
Scraping pages: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10

âœ… SCRAPING COMPLETED!
ğŸ“ Output: output/seo_data_example.com.xlsx
```

---

### Test Mode

Test mode limits processing to 10 items for quick validation:

```bash
# Test content optimization
python3 main.py --mode content --test

# Test SEO scraping
python3 main.py --mode scraping --test
```

**When to use**:
- First time setup
- Testing new sitemaps
- Validating configuration
- Quick checks before full run

---

## âš™ï¸ Configuration

### AI Provider Setup

**OpenAI**:
```yaml
ai:
  provider: openai
  model: gpt-4o-mini
  openai_api_key: "sk-your-key"
  openai_base_url: "https://api.openai.com/v1"
```

**Liara.ir (OpenAI-Compatible)**:
```yaml
ai:
  provider: openai_compatible
  model: openai/gpt-4o-mini
  compatible_base_url: "https://ai.liara.ir/api/PROJECT_ID/v1"
  compatible_api_key: "your-key"
```

**Azure OpenAI**:
```yaml
ai:
  provider: azure
  model: gpt-4o-mini
  azure_endpoint: "https://resource.openai.azure.com"
  azure_api_key: "your-key"
  azure_deployment: "gpt-4o-mini"
```

### App Settings

```yaml
app:
  min_position: 10              # Minimum position for opportunities
  clustering_threshold: 0.7     # Keyword clustering similarity
  max_headings_per_article: 8   # Max H2/H3 suggestions
  output_directory: "output"    # Output folder
```

---

## ğŸ”§ Troubleshooting

### "No Excel files found"
**Solution**: Copy your Search Console Excel files to the `input/` folder:
```bash
cp ~/Downloads/search_console_data.xlsx input/
```

### "Sitemap download failed"
**Solution**: The tool will retry 10 times automatically. Check:
- Internet connection
- Sitemap URL is correct and accessible
- No firewall blocking requests

### "API key not configured"
**Solution**: Edit `config.yaml` and add your actual API key:
```yaml
compatible_api_key: "actual-key-not-placeholder"
```

### "Missing required columns"
**Solution**: Ensure Excel export includes: `Top queries`, `Clicks`, `Impressions`, `CTR`, `Position`

### Scraping interrupted
**Solution**: Just run again! The tool will resume from where it stopped:
```bash
python3 main.py --mode scraping
# Select same sitemap, it will skip already scraped pages
```

---

## ğŸ“Š Example Workflows

### Workflow 1: Monthly Content Audit
```bash
# 1. Export fresh Search Console data
# 2. Run analysis
python3 main.py --mode content

# 3. Implement top 10 suggestions
# 4. Track results next month
```

### Workflow 2: Complete SEO Audit
```bash
# 1. Scrape all pages
python3 main.py --mode scraping

# 2. Export data, identify issues
# 3. Fix missing/duplicate titles
# 4. Re-scrape to verify fixes
```

### Workflow 3: New Content Strategy
```bash
# 1. Analyze Search Console
python3 main.py --mode content

# 2. Review new_content_*.xlsx
# 3. Create articles based on AI outlines
# 4. Track rankings in 30 days
```

---

# Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙØ§Ø±Ø³ÛŒ

## ğŸš€ Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹

### Û±. Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
```bash
pip install -r requirements.txt
```

### Û². Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ API
ÙØ§ÛŒÙ„ `config.yaml` Ø±Ø§ ÙˆÛŒØ±Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯:
```yaml
ai:
  provider: openai_compatible
  model: openai/gpt-4o-mini
  compatible_base_url: "https://ai.liara.ir/api/Ø´Ù†Ø§Ø³Ù‡_Ù¾Ø±ÙˆÚ˜Ù‡/v1"
  compatible_api_key: "Ú©Ù„ÛŒØ¯_API_Ø´Ù…Ø§"
```

### Û³. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
- ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ Ø±Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ `input/` Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯
- Ø¢Ø¯Ø±Ø³ sitemap Ø®ÙˆØ¯ Ø±Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯

### Û´. Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
```bash
# Ø­Ø§Ù„Øª ØªØ¹Ø§Ù…Ù„ÛŒ (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø±)
python3 main.py

# Ù…Ø³ØªÙ‚ÛŒÙ… Ø­Ø§Ù„Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§
python3 main.py --mode content

# Ø­Ø§Ù„Øª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ SEO
python3 main.py --mode scraping

# Ø­Ø§Ù„Øª ØªØ³Øª (Û±Û° Ø¢ÛŒØªÙ…)
python3 main.py --mode content --test
```

### Ûµ. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ v2.1 âœ¨

**ØªØ­Ù„ÛŒÙ„ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡:**
- AI Ø¨Ø§ Ø¯Ø±Ú© Ø¹Ù…ÛŒÙ‚ Ø§Ø² Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ
- Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ LSI Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ
- Ø³Ø§Ø®ØªØ§Ø± Ù…Ø­ØªÙˆØ§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
- ØªÙˆØ¬Ù‡ Ø¨Ù‡ Featured Snippet ÙØ§Ø±Ø³ÛŒ

**Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯:**
```bash
# Ø¯Ø± Ø§ÙˆÙ„ÛŒÙ† Ø§Ø¬Ø±Ø§ØŒ Ù†Ø§Ù… Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯
Project name: example.com

# Ø³ÛŒØ³ØªÙ… Ø®ÙˆØ¯Ú©Ø§Ø±:
# âœ… ØªÙ…Ø§Ù… Ù…Ø­ØªÙˆØ§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
# âœ… Ø§Ø² ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
# âœ… Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
# âœ… Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ Ø²Ù…Ø§Ù† Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
```

**Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´:**
```bash
ls knowledge_base/example.com/
# metadata.json              # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ
# content_history.json       # Ù…Ø­ØªÙˆØ§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡
# performance_metrics.json   # Ø¹Ù…Ù„Ú©Ø±Ø¯
# keyword_clusters.json      # Ú©Ù„Ø§Ø³ØªØ±Ù‡Ø§
```

---

## ğŸ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§

### Ø­Ø§Ù„Øª Û±: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§ (ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡) ğŸ‡®ğŸ‡·
- **ØªØ­Ù„ÛŒÙ„ Search Console**: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú¯ÙˆÚ¯Ù„
- **Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙØ±ØµØªâ€ŒÙ‡Ø§**: ÛŒØ§ÙØªÙ† Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ù¾ØªØ§Ù†Ø³ÛŒÙ„
- **AI ÙØ§Ø±Ø³ÛŒ**: 
  - Ø¯Ø±Ú© Ø¹Ù…ÛŒÙ‚ Ø§Ø² Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ùˆ Ù†Ú¯Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
  - ØªØ­Ù„ÛŒÙ„ search intent Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø§ÛŒØ±Ø§Ù†ÛŒ
  - Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ LSI Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ
  - ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø­Ù„ÛŒ
- **Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¬Ø§Ù…Ø¹**:
  - Ø¹Ù†ÙˆØ§Ù† Ùˆ Ù…ØªØ§ Ø¯ÛŒØ³Ú©Ø±ÛŒÙ¾Ø´Ù† Ø¨Ù‡ÛŒÙ†Ù‡ (Û¶Û° Ùˆ Û±Û¶Û° Ú©Ø§Ø±Ø§Ú©ØªØ±)
  - Ø³Ø§Ø®ØªØ§Ø± H2/H3 Ù…Ø·Ø§Ø¨Ù‚ Ø¨Ø§ Ø¬Ø³ØªØ¬ÙˆÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
  - Ø³ÙˆØ§Ù„Ø§Øª Ù…ØªØ¯Ø§ÙˆÙ„ (FAQ) Ø¨ÙˆÙ…ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
  - Internal linking Ø¨Ø§ anchor text ÙØ§Ø±Ø³ÛŒ
  - Schema markup Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ
- **Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯**:
  - Ø°Ø®ÛŒØ±Ù‡ Ø®ÙˆØ¯Ú©Ø§Ø± ØªÙ…Ø§Ù… Ù…Ø­ØªÙˆØ§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡
  - Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙˆÙ„ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
  - ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú¯Ø°Ø´ØªÙ‡

### Ø­Ø§Ù„Øª Û²: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ SEO
- **Scraping ØµÙØ­Ø§Øª**: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªØ§ÛŒØªÙ„ØŒ ØªÙˆØ¶ÛŒØ­Ø§ØªØŒ H1
- **Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ**: Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø§ Ú©Ù†ØªØ±Ù„ Ùˆ Ù‚Ø§Ø¨Ù„ÛŒØª ØªÙˆÙ‚Ù/Ø§Ø¯Ø§Ù…Ù‡
- **Ù¾ÛŒÚ¯ÛŒØ±ÛŒ Ù¾ÛŒØ´Ø±ÙØª**: Ù†ÙˆØ§Ø± Ù¾ÛŒØ´Ø±ÙØª Ùˆ Ø¢Ù…Ø§Ø± real-time
- **Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§**: ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯ Ø®ÙˆØ¯Ú©Ø§Ø±
- **Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¯Ø§Ù…Ù‡**: Ø§Ø¯Ø§Ù…Ù‡ Ø§Ø² Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù…ØªÙˆÙ‚Ù Ø´Ø¯Ù‡

### ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø´ØªØ±Ú©
- **Ù…Ø¯ÛŒØ±ÛŒØª Ù‡ÙˆØ´Ù…Ù†Ø¯ Sitemap**:
  - Ø°Ø®ÛŒØ±Ù‡ Ø®ÙˆØ¯Ú©Ø§Ø± (Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¬Ø¯Ø¯)
  - Û±Û° Ø¨Ø§Ø± ØªÙ„Ø§Ø´ Ø¨Ø§ backoff
  - Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² sitemap index
  - Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø³ØªÛŒ Ú©Ø§Ø±Ø¨Ø±

- **Ù…Ø¯ÛŒØ±ÛŒØª ÙØ§ÛŒÙ„**:
  - Ø§Ù†ØªØ®Ø§Ø¨ Ú†Ù†Ø¯ ÙØ§ÛŒÙ„ Ø§Ø² `input/`
  - Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§ÛŒÙ„
  - Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±

- **Ø­Ø§Ù„Øª ØªØ³Øª**: Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ù‡ Û±Û° Ø¢ÛŒØªÙ… Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
- **Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ**: Ø°Ø®ÛŒØ±Ù‡ logs Ø¯Ø± `seo_optimizer.log`
- **Ú†Ù†Ø¯ Ø§Ø±Ø§Ø¦Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡ AI**: OpenAIØŒ AzureØŒ AnthropicØŒ Ù„ÛŒØ§Ø±Ø§

---

## ğŸ“¦ Ù†ØµØ¨

### Ù¾ÛŒØ´â€ŒÙ†ÛŒØ§Ø²Ù‡Ø§
- Python 3.8 ÛŒØ§ Ø¨Ø§Ù„Ø§ØªØ±
- pip
- Ø§ØªØµØ§Ù„ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø¨Ø±Ø§ÛŒ API

### Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ
```bash
# Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
pip install -r requirements.txt

# Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
cp config.sample.yaml config.yaml

# ÙˆÛŒØ±Ø§ÛŒØ´ Ùˆ Ø§ÙØ²ÙˆØ¯Ù† Ú©Ù„ÛŒØ¯ API
nano config.yaml
```

### Ø³Ø§Ø®ØªØ§Ø± Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§
```
SEOContentAnalysis/
â”œâ”€â”€ input/              # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ Ø±Ø§ Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯
â”œâ”€â”€ sitemaps/           # sitemap Ù‡Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡
â”œâ”€â”€ output/             # Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
â”œâ”€â”€ main.py             # Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø§ØµÙ„ÛŒ
â”œâ”€â”€ config.yaml         # Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ø´Ù…Ø§
â””â”€â”€ src/                # Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø¨Ø¹
```

---

## ğŸ® Ø§Ø³ØªÙØ§Ø¯Ù‡

### Ø­Ø§Ù„Øª Û±: Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§

**Ù‡Ø¯Ù**: ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Search Console Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ù…Ø­ØªÙˆØ§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ùˆ ÛŒØ§ÙØªÙ† ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯.

**Ù…Ø±Ø§Ø­Ù„**:
1. Export Ø§Ø² Google Search Console (Performance â†’ Export)
2. Ú©Ù¾ÛŒ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¨Ù‡ Ù¾ÙˆØ´Ù‡ `input/`
3. Ø§Ø¬Ø±Ø§: `python3 main.py --mode content`
4. Ø§Ù†ØªØ®Ø§Ø¨ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§
5. ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø¢Ø¯Ø±Ø³ sitemap
6. Ù…Ù†ØªØ¸Ø± ØªØ­Ù„ÛŒÙ„ AI
7. Ø¨Ø±Ø±Ø³ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± `output/`

**ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ**:
- `improvements_[filename].xlsx` - Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ø±Ø§ÛŒ ØµÙØ­Ø§Øª Ù…ÙˆØ¬ÙˆØ¯
- `new_content_[filename].xlsx` - Ø§ÛŒØ¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§Ù„Ø§Øª Ø¬Ø¯ÛŒØ¯

---

### Ø­Ø§Ù„Øª Û²: Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ SEO

**Ù‡Ø¯Ù**: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ SEO ØªÙ…Ø§Ù… ØµÙØ­Ø§Øª Ø³Ø§ÛŒØª.

**Ù…Ø±Ø§Ø­Ù„**:
1. Ø§Ø¬Ø±Ø§: `python3 main.py --mode scraping`
2. ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø¢Ø¯Ø±Ø³ sitemap
3. Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø³ØªÙ‡ (Ù…Ø«Ù„Ø§ ÛµÛ° ØµÙØ­Ù‡)
4. Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ø± Ø¯Ø³ØªÙ‡ Ùˆ Ø§Ø¯Ø§Ù…Ù‡ ÛŒØ§ ØªÙˆÙ‚Ù
5. Ù†ØªØ§ÛŒØ¬ Ø¯Ø± `output/seo_data_[domain].xlsx`

**Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡**:
- Ø¢Ø¯Ø±Ø³ URL
- ØªÚ¯ Title
- Meta description
- Ø³Ø±ÙØµÙ„ H1
- Canonical URL
- ØªÚ¯â€ŒÙ‡Ø§ÛŒ Open Graph
- ØªÚ¯â€ŒÙ‡Ø§ÛŒ Twitter Card

---

### Ø­Ø§Ù„Øª ØªØ³Øª

Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø³Ø±ÛŒØ¹ Ø¨Ø§ Û±Û° Ø¢ÛŒØªÙ…:

```bash
# ØªØ³Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø­ØªÙˆØ§
python3 main.py --mode content --test

# ØªØ³Øª scraping
python3 main.py --mode scraping --test
```

**Ú†Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…**:
- Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡
- ØªØ³Øª sitemap Ø¬Ø¯ÛŒØ¯
- Ø¨Ø±Ø±Ø³ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ
- Ú†Ú© Ø³Ø±ÛŒØ¹ Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„

---

## ğŸ”§ Ø±ÙØ¹ Ù…Ø´Ú©Ù„Ø§Øª

### "Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯"
**Ø±Ø§Ù‡â€ŒØ­Ù„**: ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ `input/` Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯:
```bash
cp ~/Downloads/search_console_data.xlsx input/
```

### "Ø¯Ø§Ù†Ù„ÙˆØ¯ sitemap Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯"
**Ø±Ø§Ù‡â€ŒØ­Ù„**: Ø¨Ø±Ù†Ø§Ù…Ù‡ Û±Û° Ø¨Ø§Ø± ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯:
- Ø§ØªØµØ§Ù„ Ø§ÛŒÙ†ØªØ±Ù†Øª
- ØµØ­Øª Ø¢Ø¯Ø±Ø³ sitemap
- ÙØ§ÛŒØ±ÙˆØ§Ù„

### "Ú©Ù„ÛŒØ¯ API Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù†Ø´Ø¯Ù‡"
**Ø±Ø§Ù‡â€ŒØ­Ù„**: `config.yaml` Ø±Ø§ ÙˆÛŒØ±Ø§ÛŒØ´ Ú©Ù†ÛŒØ¯:
```yaml
compatible_api_key: "Ú©Ù„ÛŒØ¯-ÙˆØ§Ù‚Ø¹ÛŒ-Ù†Ù‡-placeholder"
```

### Scraping Ù…ØªÙˆÙ‚Ù Ø´Ø¯
**Ø±Ø§Ù‡â€ŒØ­Ù„**: Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯! Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø§Ø² Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù…ØªÙˆÙ‚Ù Ø´Ø¯Ù‡ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.

---

## ğŸ“„ Ù…Ø¬ÙˆØ²

MIT License

---

**Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ â¤ï¸ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ SEO**

